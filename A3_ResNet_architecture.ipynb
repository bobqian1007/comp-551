{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 128, 128, 1)\n",
      "Number of images in x_train 50000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_images = pd.read_pickle('D:/Study/Fall2019/COMP551/551A3/train_max_x.zip') \n",
    "x_train = np.reshape(train_images, (50000, 128, 128))\n",
    "x_train = x_train.reshape(x_train.shape[0], 128, 128, 1)\n",
    "\n",
    "df = pd.read_csv('D:/Study/Fall2019/COMP551/551A3/train_y_mod.csv')\n",
    "y_train = df.Label.to_numpy()\n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Reshaping the array to 4-dims so that it can work with the Keras API\n",
    "#X_train = X_train.reshape(X_train.shape[0], 128, 128, 1)\n",
    "#X_test = X_test.reshape(X_test.shape[0], 128, 128, 1)\n",
    "input_shape = (128, 128, 1)\n",
    "# Making sure that the values are float so that we can get decimal points after division\n",
    "x_train = x_train.astype('float32')\n",
    "#X_test = X_test.astype('float32')\n",
    "# Normalizing the RGB codes by dividing it to the max RGB value.\n",
    "x_train /= 255.0\n",
    "#X_test /= 255.0\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('Number of images in x_train', x_train.shape[0])\n",
    "#print('Number of images in x_test', X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train = x_train.astype('float32')\n",
    "#x_train /= 255.0\n",
    "x_train[x_train < 0.8] = 0\n",
    "x_train[x_train >= 0.8] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from tensorflow.python.keras import backend\n",
    "from tensorflow.python.keras import initializers\n",
    "from tensorflow.python.keras import layers\n",
    "from tensorflow.python.keras import models\n",
    "from tensorflow.python.keras import regularizers\n",
    "#from official.vision.image_classification import imagenet_preprocessing\n",
    "\n",
    "L2_WEIGHT_DECAY = 1e-4\n",
    "BATCH_NORM_DECAY = 0.9\n",
    "BATCH_NORM_EPSILON = 1e-5\n",
    "\n",
    "\n",
    "def _gen_l2_regularizer(use_l2_regularizer=True):\n",
    "  return regularizers.l2(L2_WEIGHT_DECAY) if use_l2_regularizer else None\n",
    "\n",
    "\n",
    "def identity_block(input_tensor,\n",
    "                   kernel_size,\n",
    "                   filters,\n",
    "                   stage,\n",
    "                   block,\n",
    "                   use_l2_regularizer=True):\n",
    "  \"\"\"The identity block is the block that has no conv layer at shortcut.\n",
    "  Args:\n",
    "    input_tensor: input tensor\n",
    "    kernel_size: default 3, the kernel size of middle conv layer at main path\n",
    "    filters: list of integers, the filters of 3 conv layer at main path\n",
    "    stage: integer, current stage label, used for generating layer names\n",
    "    block: 'a','b'..., current block label, used for generating layer names\n",
    "    use_l2_regularizer: whether to use L2 regularizer on Conv layer.\n",
    "  Returns:\n",
    "    Output tensor for the block.\n",
    "  \"\"\"\n",
    "  filters1, filters2, filters3 = filters\n",
    "  if backend.image_data_format() == 'channels_last':\n",
    "    bn_axis = 3\n",
    "  else:\n",
    "    bn_axis = 1\n",
    "  conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "  bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "\n",
    "  x = layers.Conv2D(\n",
    "      filters1, (1, 1),\n",
    "      use_bias=False,\n",
    "      kernel_initializer='he_normal',\n",
    "      kernel_regularizer=_gen_l2_regularizer(use_l2_regularizer),\n",
    "      name=conv_name_base + '2a')(\n",
    "          input_tensor)\n",
    "  x = layers.BatchNormalization(\n",
    "      axis=bn_axis,\n",
    "      momentum=BATCH_NORM_DECAY,\n",
    "      epsilon=BATCH_NORM_EPSILON,\n",
    "      name=bn_name_base + '2a')(\n",
    "          x)\n",
    "  x = layers.Activation('relu')(x)\n",
    "\n",
    "  x = layers.Conv2D(\n",
    "      filters2,\n",
    "      kernel_size,\n",
    "      padding='same',\n",
    "      use_bias=False,\n",
    "      kernel_initializer='he_normal',\n",
    "      kernel_regularizer=_gen_l2_regularizer(use_l2_regularizer),\n",
    "      name=conv_name_base + '2b')(\n",
    "          x)\n",
    "  x = layers.BatchNormalization(\n",
    "      axis=bn_axis,\n",
    "      momentum=BATCH_NORM_DECAY,\n",
    "      epsilon=BATCH_NORM_EPSILON,\n",
    "      name=bn_name_base + '2b')(\n",
    "          x)\n",
    "  x = layers.Activation('relu')(x)\n",
    "\n",
    "  x = layers.Conv2D(\n",
    "      filters3, (1, 1),\n",
    "      use_bias=False,\n",
    "      kernel_initializer='he_normal',\n",
    "      kernel_regularizer=_gen_l2_regularizer(use_l2_regularizer),\n",
    "      name=conv_name_base + '2c')(\n",
    "          x)\n",
    "  x = layers.BatchNormalization(\n",
    "      axis=bn_axis,\n",
    "      momentum=BATCH_NORM_DECAY,\n",
    "      epsilon=BATCH_NORM_EPSILON,\n",
    "      name=bn_name_base + '2c')(\n",
    "          x)\n",
    "\n",
    "  x = layers.add([x, input_tensor])\n",
    "  x = layers.Activation('relu')(x)\n",
    "  return x\n",
    "\n",
    "\n",
    "def conv_block(input_tensor,\n",
    "               kernel_size,\n",
    "               filters,\n",
    "               stage,\n",
    "               block,\n",
    "               strides=(2, 2),\n",
    "               use_l2_regularizer=True):\n",
    "  \"\"\"A block that has a conv layer at shortcut.\n",
    "  Note that from stage 3,\n",
    "  the second conv layer at main path is with strides=(2, 2)\n",
    "  And the shortcut should have strides=(2, 2) as well\n",
    "  Args:\n",
    "    input_tensor: input tensor\n",
    "    kernel_size: default 3, the kernel size of middle conv layer at main path\n",
    "    filters: list of integers, the filters of 3 conv layer at main path\n",
    "    stage: integer, current stage label, used for generating layer names\n",
    "    block: 'a','b'..., current block label, used for generating layer names\n",
    "    strides: Strides for the second conv layer in the block.\n",
    "    use_l2_regularizer: whether to use L2 regularizer on Conv layer.\n",
    "  Returns:\n",
    "    Output tensor for the block.\n",
    "  \"\"\"\n",
    "  filters1, filters2, filters3 = filters\n",
    "  if backend.image_data_format() == 'channels_last':\n",
    "    bn_axis = 3\n",
    "  else:\n",
    "    bn_axis = 1\n",
    "  conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "  bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "\n",
    "  x = layers.Conv2D(\n",
    "      filters1, (1, 1),\n",
    "      use_bias=False,\n",
    "      kernel_initializer='he_normal',\n",
    "      kernel_regularizer=_gen_l2_regularizer(use_l2_regularizer),\n",
    "      name=conv_name_base + '2a')(\n",
    "          input_tensor)\n",
    "  x = layers.BatchNormalization(\n",
    "      axis=bn_axis,\n",
    "      momentum=BATCH_NORM_DECAY,\n",
    "      epsilon=BATCH_NORM_EPSILON,\n",
    "      name=bn_name_base + '2a')(\n",
    "          x)\n",
    "  x = layers.Activation('relu')(x)\n",
    "\n",
    "  x = layers.Conv2D(\n",
    "      filters2,\n",
    "      kernel_size,\n",
    "      strides=strides,\n",
    "      padding='same',\n",
    "      use_bias=False,\n",
    "      kernel_initializer='he_normal',\n",
    "      kernel_regularizer=_gen_l2_regularizer(use_l2_regularizer),\n",
    "      name=conv_name_base + '2b')(\n",
    "          x)\n",
    "  x = layers.BatchNormalization(\n",
    "      axis=bn_axis,\n",
    "      momentum=BATCH_NORM_DECAY,\n",
    "      epsilon=BATCH_NORM_EPSILON,\n",
    "      name=bn_name_base + '2b')(\n",
    "          x)\n",
    "  x = layers.Activation('relu')(x)\n",
    "\n",
    "  x = layers.Conv2D(\n",
    "      filters3, (1, 1),\n",
    "      use_bias=False,\n",
    "      kernel_initializer='he_normal',\n",
    "      kernel_regularizer=_gen_l2_regularizer(use_l2_regularizer),\n",
    "      name=conv_name_base + '2c')(\n",
    "          x)\n",
    "  x = layers.BatchNormalization(\n",
    "      axis=bn_axis,\n",
    "      momentum=BATCH_NORM_DECAY,\n",
    "      epsilon=BATCH_NORM_EPSILON,\n",
    "      name=bn_name_base + '2c')(\n",
    "          x)\n",
    "\n",
    "  shortcut = layers.Conv2D(\n",
    "      filters3, (1, 1),\n",
    "      strides=strides,\n",
    "      use_bias=False,\n",
    "      kernel_initializer='he_normal',\n",
    "      kernel_regularizer=_gen_l2_regularizer(use_l2_regularizer),\n",
    "      name=conv_name_base + '1')(\n",
    "          input_tensor)\n",
    "  shortcut = layers.BatchNormalization(\n",
    "      axis=bn_axis,\n",
    "      momentum=BATCH_NORM_DECAY,\n",
    "      epsilon=BATCH_NORM_EPSILON,\n",
    "      name=bn_name_base + '1')(\n",
    "          shortcut)\n",
    "\n",
    "  x = layers.add([x, shortcut])\n",
    "  x = layers.Activation('relu')(x)\n",
    "  return x\n",
    "\n",
    "\n",
    "def resnet50(num_classes,\n",
    "             batch_size=None,\n",
    "             use_l2_regularizer=True,\n",
    "             rescale_inputs=False):\n",
    "  \"\"\"Instantiates the ResNet50 architecture.\n",
    "  Args:\n",
    "    num_classes: `int` number of classes for image classification.\n",
    "    batch_size: Size of the batches for each step.\n",
    "    use_l2_regularizer: whether to use L2 regularizer on Conv/Dense layer.\n",
    "    rescale_inputs: whether to rescale inputs from 0 to 1.\n",
    "  Returns:\n",
    "      A Keras model instance.\n",
    "  \"\"\"\n",
    "  input_shape = (128, 128, 1)\n",
    "  img_input = layers.Input(shape=input_shape, batch_size=batch_size)\n",
    "  if rescale_inputs:\n",
    "    # Hub image modules expect inputs in the range [0, 1]. This rescales these\n",
    "    # inputs to the range expected by the trained model.\n",
    "    x = layers.Lambda(\n",
    "        lambda x: x * 255.0 - backend.constant(\n",
    "            imagenet_preprocessing.CHANNEL_MEANS,\n",
    "            shape=[1, 1, 3],\n",
    "            dtype=x.dtype),\n",
    "        name='rescale')(\n",
    "            img_input)\n",
    "  else:\n",
    "    x = img_input\n",
    "\n",
    "  if backend.image_data_format() == 'channels_first':\n",
    "    x = layers.Lambda(\n",
    "        lambda x: backend.permute_dimensions(x, (0, 3, 1, 2)),\n",
    "        name='transpose')(x)\n",
    "    bn_axis = 1\n",
    "  else:  # channels_last\n",
    "    bn_axis = 3\n",
    "\n",
    "  x = layers.ZeroPadding2D(padding=(3, 3), name='conv1_pad')(x)\n",
    "  x = layers.Conv2D(\n",
    "      64, (7, 7),\n",
    "      strides=(2, 2),\n",
    "      padding='valid',\n",
    "      use_bias=False,\n",
    "      kernel_initializer='he_normal',\n",
    "      kernel_regularizer=_gen_l2_regularizer(use_l2_regularizer),\n",
    "      name='conv1')(\n",
    "          x)\n",
    "  x = layers.BatchNormalization(\n",
    "      axis=bn_axis,\n",
    "      momentum=BATCH_NORM_DECAY,\n",
    "      epsilon=BATCH_NORM_EPSILON,\n",
    "      name='bn_conv1')(\n",
    "          x)\n",
    "  x = layers.Activation('relu')(x)\n",
    "  x = layers.MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n",
    "\n",
    "  x = conv_block(\n",
    "      x,\n",
    "      3, [64, 64, 256],\n",
    "      stage=2,\n",
    "      block='a',\n",
    "      strides=(1, 1),\n",
    "      use_l2_regularizer=use_l2_regularizer)\n",
    "  x = identity_block(\n",
    "      x,\n",
    "      3, [64, 64, 256],\n",
    "      stage=2,\n",
    "      block='b',\n",
    "      use_l2_regularizer=use_l2_regularizer)\n",
    "  x = identity_block(\n",
    "      x,\n",
    "      3, [64, 64, 256],\n",
    "      stage=2,\n",
    "      block='c',\n",
    "      use_l2_regularizer=use_l2_regularizer)\n",
    "\n",
    "  x = conv_block(\n",
    "      x,\n",
    "      3, [128, 128, 512],\n",
    "      stage=3,\n",
    "      block='a',\n",
    "      use_l2_regularizer=use_l2_regularizer)\n",
    "  x = identity_block(\n",
    "      x,\n",
    "      3, [128, 128, 512],\n",
    "      stage=3,\n",
    "      block='b',\n",
    "      use_l2_regularizer=use_l2_regularizer)\n",
    "  x = identity_block(\n",
    "      x,\n",
    "      3, [128, 128, 512],\n",
    "      stage=3,\n",
    "      block='c',\n",
    "      use_l2_regularizer=use_l2_regularizer)\n",
    "  x = identity_block(\n",
    "      x,\n",
    "      3, [128, 128, 512],\n",
    "      stage=3,\n",
    "      block='d',\n",
    "      use_l2_regularizer=use_l2_regularizer)\n",
    "\n",
    "  x = conv_block(\n",
    "      x,\n",
    "      3, [256, 256, 1024],\n",
    "      stage=4,\n",
    "      block='a',\n",
    "      use_l2_regularizer=use_l2_regularizer)\n",
    "  x = identity_block(\n",
    "      x,\n",
    "      3, [256, 256, 1024],\n",
    "      stage=4,\n",
    "      block='b',\n",
    "      use_l2_regularizer=use_l2_regularizer)\n",
    "  x = identity_block(\n",
    "      x,\n",
    "      3, [256, 256, 1024],\n",
    "      stage=4,\n",
    "      block='c',\n",
    "      use_l2_regularizer=use_l2_regularizer)\n",
    "  x = identity_block(\n",
    "      x,\n",
    "      3, [256, 256, 1024],\n",
    "      stage=4,\n",
    "      block='d',\n",
    "      use_l2_regularizer=use_l2_regularizer)\n",
    "  x = identity_block(\n",
    "      x,\n",
    "      3, [256, 256, 1024],\n",
    "      stage=4,\n",
    "      block='e',\n",
    "      use_l2_regularizer=use_l2_regularizer)\n",
    "  x = identity_block(\n",
    "      x,\n",
    "      3, [256, 256, 1024],\n",
    "      stage=4,\n",
    "      block='f',\n",
    "      use_l2_regularizer=use_l2_regularizer)\n",
    "\n",
    "  x = conv_block(\n",
    "      x,\n",
    "      3, [512, 512, 2048],\n",
    "      stage=5,\n",
    "      block='a',\n",
    "      use_l2_regularizer=use_l2_regularizer)\n",
    "  x = identity_block(\n",
    "      x,\n",
    "      3, [512, 512, 2048],\n",
    "      stage=5,\n",
    "      block='b',\n",
    "      use_l2_regularizer=use_l2_regularizer)\n",
    "  x = identity_block(\n",
    "      x,\n",
    "      3, [512, 512, 2048],\n",
    "      stage=5,\n",
    "      block='c',\n",
    "      use_l2_regularizer=use_l2_regularizer)\n",
    "\n",
    "  rm_axes = [1, 2] if backend.image_data_format() == 'channels_last' else [2, 3]\n",
    "  x = layers.Lambda(lambda x: backend.mean(x, rm_axes), name='reduce_mean')(x)\n",
    "  x = layers.Dense(\n",
    "      num_classes,\n",
    "      kernel_initializer=initializers.RandomNormal(stddev=0.01),\n",
    "      kernel_regularizer=_gen_l2_regularizer(use_l2_regularizer),\n",
    "      bias_regularizer=_gen_l2_regularizer(use_l2_regularizer),\n",
    "      name='fc1000')(\n",
    "          x)\n",
    "\n",
    "  # A softmax that is followed by the model loss must be done cannot be done\n",
    "  # in float16 due to numeric issues. So we pass dtype=float32.\n",
    "  x = layers.Activation('softmax', dtype='float32')(x)\n",
    "\n",
    "  # Create model.\n",
    "  return models.Model(img_input, x, name='resnet50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda\\AnacondaInstall\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\initializers.py:143: calling RandomNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "model = resnet50(10,\n",
    "             batch_size=32,\n",
    "             use_l2_regularizer=True,\n",
    "             rescale_inputs=False)\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', \n",
    "              loss='sparse_categorical_crossentropy', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "49984/49984 [==============================] - 195s 4ms/sample - loss: 3.3119 - acc: 0.5783\n",
      "49984/49984 [==============================] - 184s 4ms/sample - loss: 1.1242 - acc: 0.8346\n",
      "epoch 1\n",
      "49984/49984 [==============================] - 184s 4ms/sample - loss: 0.8617 - acc: 0.8723\n",
      "49984/49984 [==============================] - 183s 4ms/sample - loss: 0.7272 - acc: 0.8965\n",
      "epoch 2\n",
      "49984/49984 [==============================] - 183s 4ms/sample - loss: 0.6529 - acc: 0.9114\n",
      "49984/49984 [==============================] - 183s 4ms/sample - loss: 0.6038 - acc: 0.9194\n",
      "epoch 3\n",
      "49984/49984 [==============================] - 183s 4ms/sample - loss: 0.5622 - acc: 0.9255\n",
      "49984/49984 [==============================] - 182s 4ms/sample - loss: 0.5189 - acc: 0.9317\n",
      "epoch 4\n",
      "49984/49984 [==============================] - 182s 4ms/sample - loss: 0.4756 - acc: 0.9383\n",
      "49984/49984 [==============================] - 182s 4ms/sample - loss: 0.4416 - acc: 0.9442\n",
      "epoch 5\n",
      "49984/49984 [==============================] - 182s 4ms/sample - loss: 0.4137 - acc: 0.9494\n",
      "49984/49984 [==============================] - 181s 4ms/sample - loss: 0.3943 - acc: 0.9520\n",
      "epoch 6\n",
      "49984/49984 [==============================] - 181s 4ms/sample - loss: 0.3794 - acc: 0.9535\n",
      "49984/49984 [==============================] - 181s 4ms/sample - loss: 0.3576 - acc: 0.9572\n",
      "epoch 7\n",
      "49984/49984 [==============================] - 181s 4ms/sample - loss: 0.3454 - acc: 0.9581\n",
      "49984/49984 [==============================] - 181s 4ms/sample - loss: 0.3351 - acc: 0.9595\n",
      "epoch 8\n",
      "49984/49984 [==============================] - 181s 4ms/sample - loss: 0.3277 - acc: 0.9602\n",
      "49984/49984 [==============================] - 181s 4ms/sample - loss: 0.3160 - acc: 0.9621\n",
      "epoch 9\n",
      "49984/49984 [==============================] - 180s 4ms/sample - loss: 0.3106 - acc: 0.9622\n",
      "49984/49984 [==============================] - 180s 4ms/sample - loss: 0.3013 - acc: 0.9648\n",
      "epoch 10\n",
      "49984/49984 [==============================] - 180s 4ms/sample - loss: 0.2959 - acc: 0.9641\n",
      "49984/49984 [==============================] - 180s 4ms/sample - loss: 0.2882 - acc: 0.9651\n",
      "epoch 11\n",
      "49984/49984 [==============================] - 180s 4ms/sample - loss: 0.2827 - acc: 0.9666\n",
      "49984/49984 [==============================] - 180s 4ms/sample - loss: 0.2812 - acc: 0.9660\n",
      "epoch 12\n",
      "49984/49984 [==============================] - 180s 4ms/sample - loss: 0.2687 - acc: 0.9680\n",
      "49984/49984 [==============================] - 180s 4ms/sample - loss: 0.2671 - acc: 0.9681\n",
      "epoch 13\n",
      "49984/49984 [==============================] - 180s 4ms/sample - loss: 0.2654 - acc: 0.9680\n",
      "49984/49984 [==============================] - 180s 4ms/sample - loss: 0.2567 - acc: 0.9698\n",
      "epoch 14\n",
      "49984/49984 [==============================] - 180s 4ms/sample - loss: 0.2572 - acc: 0.9690\n",
      "49984/49984 [==============================] - 180s 4ms/sample - loss: 0.2554 - acc: 0.9693\n",
      "epoch 15\n",
      "49984/49984 [==============================] - 180s 4ms/sample - loss: 0.2458 - acc: 0.9714\n",
      "49984/49984 [==============================] - 180s 4ms/sample - loss: 0.2498 - acc: 0.9701\n",
      "epoch 16\n",
      "49984/49984 [==============================] - 180s 4ms/sample - loss: 0.2443 - acc: 0.9715\n",
      "49984/49984 [==============================] - 180s 4ms/sample - loss: 0.2390 - acc: 0.9715\n",
      "epoch 17\n",
      "49984/49984 [==============================] - 180s 4ms/sample - loss: 0.2399 - acc: 0.9716\n",
      "49984/49984 [==============================] - 180s 4ms/sample - loss: 0.2298 - acc: 0.9733\n",
      "epoch 18\n",
      "49984/49984 [==============================] - 180s 4ms/sample - loss: 0.2285 - acc: 0.9732\n",
      "49984/49984 [==============================] - 180s 4ms/sample - loss: 0.2295 - acc: 0.9731\n",
      "epoch 19\n",
      "49984/49984 [==============================] - 180s 4ms/sample - loss: 0.2216 - acc: 0.9735\n",
      "49984/49984 [==============================] - 180s 4ms/sample - loss: 0.2219 - acc: 0.9731\n",
      "epoch 20\n",
      "49984/49984 [==============================] - 180s 4ms/sample - loss: 0.2237 - acc: 0.9741\n",
      "49984/49984 [==============================] - 180s 4ms/sample - loss: 0.2181 - acc: 0.9755\n",
      "epoch 21\n",
      "49984/49984 [==============================] - 180s 4ms/sample - loss: 0.2169 - acc: 0.9754\n",
      "49984/49984 [==============================] - 180s 4ms/sample - loss: 0.2160 - acc: 0.9747\n",
      "epoch 22\n",
      "49984/49984 [==============================] - 180s 4ms/sample - loss: 0.2162 - acc: 0.9748\n",
      "49984/49984 [==============================] - 180s 4ms/sample - loss: 0.2101 - acc: 0.9757\n",
      "epoch 23\n",
      "49984/49984 [==============================] - 180s 4ms/sample - loss: 0.2123 - acc: 0.9746\n",
      "49984/49984 [==============================] - 180s 4ms/sample - loss: 0.2095 - acc: 0.9760\n",
      "epoch 24\n",
      "49984/49984 [==============================] - 180s 4ms/sample - loss: 0.2098 - acc: 0.9754\n",
      "49984/49984 [==============================] - 180s 4ms/sample - loss: 0.2153 - acc: 0.9745\n",
      "epoch 25\n",
      "49984/49984 [==============================] - 180s 4ms/sample - loss: 0.2062 - acc: 0.9761\n",
      "49984/49984 [==============================] - 180s 4ms/sample - loss: 0.2060 - acc: 0.9767\n",
      "epoch 26\n",
      "49984/49984 [==============================] - 180s 4ms/sample - loss: 0.2070 - acc: 0.9754\n",
      "49984/49984 [==============================] - 180s 4ms/sample - loss: 0.2037 - acc: 0.9763\n",
      "epoch 27\n",
      "49984/49984 [==============================] - 180s 4ms/sample - loss: 0.1969 - acc: 0.9787\n",
      "49984/49984 [==============================] - 180s 4ms/sample - loss: 0.1981 - acc: 0.9777\n",
      "epoch 28\n",
      "49984/49984 [==============================] - 180s 4ms/sample - loss: 0.2065 - acc: 0.9755\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1935 - acc: 0.9789\n",
      "epoch 29\n",
      "49984/49984 [==============================] - 180s 4ms/sample - loss: 0.1957 - acc: 0.9783\n",
      "49984/49984 [==============================] - 180s 4ms/sample - loss: 0.1932 - acc: 0.9784\n",
      "epoch 30\n",
      "49984/49984 [==============================] - 180s 4ms/sample - loss: 0.1957 - acc: 0.9780\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1917 - acc: 0.9790\n",
      "epoch 31\n",
      "49984/49984 [==============================] - 180s 4ms/sample - loss: 0.1933 - acc: 0.9782\n",
      "49984/49984 [==============================] - 180s 4ms/sample - loss: 0.1894 - acc: 0.9791\n",
      "epoch 32\n",
      "49984/49984 [==============================] - 180s 4ms/sample - loss: 0.1941 - acc: 0.9784\n",
      "49984/49984 [==============================] - 180s 4ms/sample - loss: 0.1886 - acc: 0.9801\n",
      "epoch 33\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1908 - acc: 0.9792\n",
      "49984/49984 [==============================] - 180s 4ms/sample - loss: 0.1859 - acc: 0.9802\n",
      "epoch 34\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1848 - acc: 0.9804\n",
      "49984/49984 [==============================] - 180s 4ms/sample - loss: 0.1858 - acc: 0.9802\n",
      "epoch 35\n",
      "49984/49984 [==============================] - 180s 4ms/sample - loss: 0.1872 - acc: 0.9795\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1855 - acc: 0.9799\n",
      "epoch 36\n",
      "49984/49984 [==============================] - 183s 4ms/sample - loss: 0.1889 - acc: 0.9798\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1837 - acc: 0.9797\n",
      "epoch 37\n",
      "49984/49984 [==============================] - 180s 4ms/sample - loss: 0.1842 - acc: 0.9807\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1860 - acc: 0.9796\n",
      "epoch 38\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1799 - acc: 0.9811\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1862 - acc: 0.9792\n",
      "epoch 39\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1812 - acc: 0.9812\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1818 - acc: 0.9801\n",
      "epoch 40\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1817 - acc: 0.9804\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1778 - acc: 0.9811\n",
      "epoch 41\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1803 - acc: 0.9799\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1829 - acc: 0.9794\n",
      "epoch 42\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1784 - acc: 0.9812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49984/49984 [==============================] - 180s 4ms/sample - loss: 0.1767 - acc: 0.9813\n",
      "epoch 43\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1741 - acc: 0.9815\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1857 - acc: 0.9793\n",
      "epoch 44\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1773 - acc: 0.9809\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1752 - acc: 0.9813\n"
     ]
    }
   ],
   "source": [
    "for i in range(45):\n",
    "    print(\"epoch\", i)\n",
    "    model.fit(x=x_train[0:49984] ,y=y_train[0:49984], epochs=1, batch_size = 32)\n",
    "    model.fit(x=x_train[16:50000] ,y=y_train[16:50000], epochs=1, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 45\n",
      "49984/49984 [==============================] - 178s 4ms/sample - loss: 0.1755 - acc: 0.9816\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1728 - acc: 0.9827\n",
      "epoch 46\n",
      "49984/49984 [==============================] - 180s 4ms/sample - loss: 0.1782 - acc: 0.9804\n",
      "49984/49984 [==============================] - 180s 4ms/sample - loss: 0.1725 - acc: 0.9817\n",
      "epoch 47\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1754 - acc: 0.9816\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1726 - acc: 0.9815\n",
      "epoch 48\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1737 - acc: 0.9814\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1721 - acc: 0.9818\n",
      "epoch 49\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1658 - acc: 0.9834\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1740 - acc: 0.9813\n",
      "epoch 50\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1705 - acc: 0.9823\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1692 - acc: 0.9824\n",
      "epoch 51\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1707 - acc: 0.9818\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1695 - acc: 0.9821\n",
      "epoch 52\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1705 - acc: 0.9818\n",
      "49984/49984 [==============================] - 180s 4ms/sample - loss: 0.1692 - acc: 0.9822\n",
      "epoch 53\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1676 - acc: 0.9827\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1700 - acc: 0.9820\n",
      "epoch 54\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1686 - acc: 0.9826\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1736 - acc: 0.9804\n",
      "epoch 55\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1671 - acc: 0.9833\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1650 - acc: 0.9827\n",
      "epoch 56\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1691 - acc: 0.9818\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1666 - acc: 0.9829\n",
      "epoch 57\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1674 - acc: 0.9828\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1700 - acc: 0.9814\n",
      "epoch 58\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1667 - acc: 0.9820\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1670 - acc: 0.9820\n",
      "epoch 59\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1695 - acc: 0.9822\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1660 - acc: 0.9823\n"
     ]
    }
   ],
   "source": [
    "for i in range(45,60):\n",
    "    print(\"epoch\", i)\n",
    "    model.fit(x=x_train[0:49984] ,y=y_train[0:49984], epochs=1, batch_size = 32)\n",
    "    model.fit(x=x_train[16:50000] ,y=y_train[16:50000], epochs=1, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_images = pd.read_pickle('D:/Study/Fall2019/COMP551/551A3/test_max_x.zip') \n",
    "x_test = np.reshape(result_images, (10000, 128, 128))\n",
    "x_test = np.reshape(x_test, (10000, 128, 128, 1))\n",
    "\n",
    "x_test = x_test.astype('float32')\n",
    "x_test /= 255.0\n",
    "\n",
    "x_test[x_test < 0.8] = 0\n",
    "x_test[x_test >= 0.8] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_result21 = model.predict(x=x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49984/49984 [==============================] - 178s 4ms/sample - loss: 0.1459 - acc: 0.9853\n",
      "49984/49984 [==============================] - 180s 4ms/sample - loss: 0.1446 - acc: 0.9851\n",
      "49984/49984 [==============================] - 181s 4ms/sample - loss: 0.1378 - acc: 0.9870\n",
      "49984/49984 [==============================] - 181s 4ms/sample - loss: 0.1436 - acc: 0.9849\n",
      "49984/49984 [==============================] - 181s 4ms/sample - loss: 0.1415 - acc: 0.9856\n",
      "49984/49984 [==============================] - 180s 4ms/sample - loss: 0.1483 - acc: 0.9839\n",
      "49984/49984 [==============================] - 180s 4ms/sample - loss: 0.1417 - acc: 0.9858\n",
      "49984/49984 [==============================] - 180s 4ms/sample - loss: 0.1471 - acc: 0.9846\n",
      "49984/49984 [==============================] - 180s 4ms/sample - loss: 0.1453 - acc: 0.9846\n"
     ]
    }
   ],
   "source": [
    "model.fit(x=x_train[0:49984] ,y=y_train[0:49984], epochs=1, batch_size = 32)\n",
    "y_result22 = model.predict(x=x_test)\n",
    "\n",
    "model.fit(x=x_train[16:50000] ,y=y_train[16:50000], epochs=1, batch_size = 32)\n",
    "y_result23 = model.predict(x=x_test)\n",
    "\n",
    "model.fit(x=x_train[0:49984] ,y=y_train[0:49984], epochs=1, batch_size = 32)\n",
    "y_result24 = model.predict(x=x_test)\n",
    "\n",
    "model.fit(x=x_train[16:50000] ,y=y_train[16:50000], epochs=1, batch_size = 32)\n",
    "y_result25 = model.predict(x=x_test)\n",
    "\n",
    "model.fit(x=x_train[0:49984] ,y=y_train[0:49984], epochs=1, batch_size = 32)\n",
    "y_result26 = model.predict(x=x_test)\n",
    "\n",
    "model.fit(x=x_train[16:50000] ,y=y_train[16:50000], epochs=1, batch_size = 32)\n",
    "y_result27 = model.predict(x=x_test)\n",
    "\n",
    "model.fit(x=x_train[0:49984] ,y=y_train[0:49984], epochs=1, batch_size = 32)\n",
    "y_result28 = model.predict(x=x_test)\n",
    "\n",
    "model.fit(x=x_train[16:50000] ,y=y_train[16:50000], epochs=1, batch_size = 32)\n",
    "y_result29 = model.predict(x=x_test)\n",
    "\n",
    "model.fit(x=x_train[0:49984] ,y=y_train[0:49984], epochs=1, batch_size = 32)\n",
    "y_result30 = model.predict(x=x_test)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_frequent(List): \n",
    "    counter = 0\n",
    "    num = List[0] \n",
    "      \n",
    "    for i in List: \n",
    "        curr_frequency = List.count(i) \n",
    "        if(curr_frequency> counter): \n",
    "            counter = curr_frequency \n",
    "            num = i \n",
    "  \n",
    "    return num \n",
    "\n",
    "\n",
    "result1=[]\n",
    "result2=[]\n",
    "result3=[]\n",
    "result4=[]\n",
    "result5=[]\n",
    "result6=[]\n",
    "result7=[]\n",
    "result8=[]\n",
    "result9=[]\n",
    "result10=[]\n",
    "\n",
    "\n",
    "\n",
    "for i in range(10000):\n",
    "    result1.append(np.argmax(y_result21[i]))\n",
    "    result2.append(np.argmax(y_result22[i]))\n",
    "    result3.append(np.argmax(y_result23[i]))\n",
    "    result4.append(np.argmax(y_result24[i]))\n",
    "    result5.append(np.argmax(y_result25[i]))\n",
    "    result6.append(np.argmax(y_result26[i]))\n",
    "    result7.append(np.argmax(y_result27[i]))\n",
    "    result8.append(np.argmax(y_result28[i]))\n",
    "    result9.append(np.argmax(y_result29[i]))\n",
    "    result10.append(np.argmax(y_result30[i]))\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalResult = []\n",
    "\n",
    "for i in range(10000):\n",
    "    helper = []\n",
    "    helper.append(result1[i])\n",
    "    helper.append(result2[i])\n",
    "    helper.append(result3[i])\n",
    "    helper.append(result4[i])\n",
    "    helper.append(result5[i])\n",
    "    helper.append(result6[i])\n",
    "    helper.append(result7[i])\n",
    "    helper.append(result8[i])\n",
    "    helper.append(result9[i])\n",
    "    helper.append(result10[i])\n",
    "    \n",
    "    finalResult.append(most_frequent(helper))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('D:/Study/Fall2019/COMP551/551A3/submitWedFinal2.csv', mode='w') as writingFile:\n",
    "    writer = csv.writer(writingFile)\n",
    "    writer.writerow(['id', 'Label'])\n",
    "    for index in range(10000):\n",
    "        writer.writerow([index, finalResult[index]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1413 - acc: 0.9858\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x200a6033a88>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=x_train[16:50000] ,y=y_train[16:50000], epochs=1, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('baby140.h5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 70\n",
      "49984/49984 [==============================] - 178s 4ms/sample - loss: 0.1680 - acc: 0.9817\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1626 - acc: 0.9828\n",
      "epoch 71\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1632 - acc: 0.9832\n",
      "49984/49984 [==============================] - 180s 4ms/sample - loss: 0.1624 - acc: 0.9832\n",
      "epoch 72\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1636 - acc: 0.9823\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1598 - acc: 0.9837\n",
      "epoch 73\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1683 - acc: 0.9815\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1641 - acc: 0.9831\n",
      "epoch 74\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1607 - acc: 0.9829\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1676 - acc: 0.9816\n",
      "epoch 75\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1607 - acc: 0.9835\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1625 - acc: 0.9834\n",
      "epoch 76\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1616 - acc: 0.9823\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1604 - acc: 0.9831\n",
      "epoch 77\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1574 - acc: 0.9838\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1575 - acc: 0.9844\n",
      "epoch 78\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1601 - acc: 0.9838\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1604 - acc: 0.9838\n",
      "epoch 79\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1620 - acc: 0.9825\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1559 - acc: 0.9846\n",
      "epoch 80\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1597 - acc: 0.9830\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1655 - acc: 0.9823\n",
      "epoch 81\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1527 - acc: 0.9851\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1584 - acc: 0.9831\n",
      "epoch 82\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1638 - acc: 0.9826\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1571 - acc: 0.9838\n",
      "epoch 83\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1593 - acc: 0.9839\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1528 - acc: 0.9850\n",
      "epoch 84\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1604 - acc: 0.9827\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1555 - acc: 0.9841\n",
      "epoch 85\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1586 - acc: 0.9835\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1562 - acc: 0.9839\n",
      "epoch 86\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1556 - acc: 0.9841\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1585 - acc: 0.9834\n",
      "epoch 87\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1568 - acc: 0.9836\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1559 - acc: 0.9836\n",
      "epoch 88\n",
      "49984/49984 [==============================] - 181s 4ms/sample - loss: 0.1578 - acc: 0.9837\n",
      "49984/49984 [==============================] - 181s 4ms/sample - loss: 0.1551 - acc: 0.9839\n",
      "epoch 89\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1537 - acc: 0.9840\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1560 - acc: 0.9840\n",
      "epoch 90\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1581 - acc: 0.9834\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1505 - acc: 0.9851\n",
      "epoch 91\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1661 - acc: 0.9811\n",
      "49984/49984 [==============================] - 181s 4ms/sample - loss: 0.1510 - acc: 0.9854\n",
      "epoch 92\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1579 - acc: 0.9831\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1509 - acc: 0.9852\n",
      "epoch 93\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1555 - acc: 0.9839\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1557 - acc: 0.9836\n",
      "epoch 94\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1525 - acc: 0.9845\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1594 - acc: 0.9833\n",
      "epoch 95\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1531 - acc: 0.9848\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1535 - acc: 0.9833\n",
      "epoch 96\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1538 - acc: 0.9837\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1578 - acc: 0.9831\n",
      "epoch 97\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1517 - acc: 0.9845\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1551 - acc: 0.9836\n",
      "epoch 98\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1544 - acc: 0.9831\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1524 - acc: 0.9839\n",
      "epoch 99\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1480 - acc: 0.9857\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1550 - acc: 0.9834\n",
      "epoch 100\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1505 - acc: 0.9847\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1495 - acc: 0.9851\n",
      "epoch 101\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1542 - acc: 0.9837\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1500 - acc: 0.9852\n",
      "epoch 102\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1527 - acc: 0.9841\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1494 - acc: 0.9850\n",
      "epoch 103\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1595 - acc: 0.9828\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1489 - acc: 0.9850\n",
      "epoch 104\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1476 - acc: 0.9853\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1520 - acc: 0.9839\n",
      "epoch 105\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1515 - acc: 0.9838\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1500 - acc: 0.9842\n",
      "epoch 106\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1483 - acc: 0.9851\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1494 - acc: 0.9845\n",
      "epoch 107\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1519 - acc: 0.9844\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1452 - acc: 0.9857\n",
      "epoch 108\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1500 - acc: 0.9841\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1485 - acc: 0.9851\n",
      "epoch 109\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1507 - acc: 0.9843\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1483 - acc: 0.9851\n"
     ]
    }
   ],
   "source": [
    "for i in range(70,110):\n",
    "    print(\"epoch\", i)\n",
    "    model.fit(x=x_train[0:49984] ,y=y_train[0:49984], epochs=1, batch_size = 32)\n",
    "    model.fit(x=x_train[16:50000] ,y=y_train[16:50000], epochs=1, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 110\n",
      "49984/49984 [==============================] - 178s 4ms/sample - loss: 0.1482 - acc: 0.9850\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1510 - acc: 0.9845\n",
      "epoch 111\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1485 - acc: 0.9848\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1501 - acc: 0.9846\n",
      "epoch 112\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1457 - acc: 0.9855\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1531 - acc: 0.9838\n",
      "epoch 113\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1517 - acc: 0.9843\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1497 - acc: 0.9850\n",
      "epoch 114\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1479 - acc: 0.9846\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1461 - acc: 0.9851\n",
      "epoch 115\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1524 - acc: 0.9843\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1452 - acc: 0.9857\n",
      "epoch 116\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1489 - acc: 0.9848\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1536 - acc: 0.9834\n",
      "epoch 117\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1456 - acc: 0.9848\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1425 - acc: 0.9864\n",
      "epoch 118\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1488 - acc: 0.9840\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1491 - acc: 0.9844\n",
      "epoch 119\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1442 - acc: 0.9863\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1457 - acc: 0.9853\n",
      "epoch 120\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1473 - acc: 0.9846\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1440 - acc: 0.9853\n",
      "epoch 121\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1554 - acc: 0.9829\n",
      "49984/49984 [==============================] - 181s 4ms/sample - loss: 0.1435 - acc: 0.9858\n",
      "epoch 122\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1473 - acc: 0.9846\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1469 - acc: 0.9850\n",
      "epoch 123\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1472 - acc: 0.9850\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1480 - acc: 0.9838\n",
      "epoch 124\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1457 - acc: 0.9852\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1467 - acc: 0.9845\n",
      "epoch 125\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1456 - acc: 0.9857\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1473 - acc: 0.9844\n",
      "epoch 126\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1454 - acc: 0.9853\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1460 - acc: 0.9851\n",
      "epoch 127\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1474 - acc: 0.9852\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1459 - acc: 0.9847\n",
      "epoch 128\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1450 - acc: 0.9855\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1494 - acc: 0.9840\n",
      "epoch 129\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1472 - acc: 0.9842\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1465 - acc: 0.9846\n",
      "epoch 130\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1455 - acc: 0.9855\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1458 - acc: 0.9854\n",
      "epoch 131\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1462 - acc: 0.9854\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1465 - acc: 0.9853\n",
      "epoch 132\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1495 - acc: 0.9841\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1429 - acc: 0.9862\n",
      "epoch 133\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1404 - acc: 0.9869\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1458 - acc: 0.9849\n",
      "epoch 134\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1432 - acc: 0.9860\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1449 - acc: 0.9850\n",
      "epoch 135\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1474 - acc: 0.9849\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1404 - acc: 0.9864\n",
      "epoch 136\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1418 - acc: 0.9857\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1422 - acc: 0.9852\n",
      "epoch 137\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1474 - acc: 0.9838\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1449 - acc: 0.9853\n",
      "epoch 138\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1417 - acc: 0.9856\n",
      "49984/49984 [==============================] - 180s 4ms/sample - loss: 0.1470 - acc: 0.9847\n",
      "epoch 139\n",
      "49984/49984 [==============================] - 180s 4ms/sample - loss: 0.1425 - acc: 0.9856\n",
      "49984/49984 [==============================] - 179s 4ms/sample - loss: 0.1432 - acc: 0.9853\n"
     ]
    }
   ],
   "source": [
    "for i in range(110,140):\n",
    "    print(\"epoch\", i)\n",
    "    model.fit(x=x_train[0:49984] ,y=y_train[0:49984], epochs=1, batch_size = 32)\n",
    "    model.fit(x=x_train[16:50000] ,y=y_train[16:50000], epochs=1, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_result = model.predict(x=x_test)\n",
    "\n",
    "result = []\n",
    "for i in range(10000):\n",
    "    result.append(np.argmax(y_result[i]))\n",
    "    \n",
    "with open('D:/Study/Fall2019/COMP551/551A3/submitWedFinal2.csv', mode='w') as writingFile:\n",
    "    writer = csv.writer(writingFile)\n",
    "    writer.writerow(['id', 'Label'])\n",
    "    for index in range(10000):\n",
    "        writer.writerow([index, result[index]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda\\AnacondaInstall\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From D:\\Anaconda\\AnacondaInstall\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From D:\\Anaconda\\AnacondaInstall\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "m2 = tf.keras.models.load_model('baby70.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"resnet50\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(32, 128, 128, 1)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (32, 134, 134, 1)    0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (32, 64, 64, 64)     3136        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bn_conv1 (BatchNormalization)   (32, 64, 64, 64)     256         conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (32, 64, 64, 64)     0           bn_conv1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (32, 32, 32, 64)     0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2a (Conv2D)         (32, 32, 32, 64)     4096        max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2a (BatchNormalizati (32, 32, 32, 64)     256         res2a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (32, 32, 32, 64)     0           bn2a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2b (Conv2D)         (32, 32, 32, 64)     36864       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2b (BatchNormalizati (32, 32, 32, 64)     256         res2a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (32, 32, 32, 64)     0           bn2a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2c (Conv2D)         (32, 32, 32, 256)    16384       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch1 (Conv2D)          (32, 32, 32, 256)    16384       max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2c (BatchNormalizati (32, 32, 32, 256)    1024        res2a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch1 (BatchNormalizatio (32, 32, 32, 256)    1024        res2a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (32, 32, 32, 256)    0           bn2a_branch2c[0][0]              \n",
      "                                                                 bn2a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (32, 32, 32, 256)    0           add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2a (Conv2D)         (32, 32, 32, 64)     16384       activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2a (BatchNormalizati (32, 32, 32, 64)     256         res2b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (32, 32, 32, 64)     0           bn2b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2b (Conv2D)         (32, 32, 32, 64)     36864       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2b (BatchNormalizati (32, 32, 32, 64)     256         res2b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (32, 32, 32, 64)     0           bn2b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2c (Conv2D)         (32, 32, 32, 256)    16384       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2c (BatchNormalizati (32, 32, 32, 256)    1024        res2b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (32, 32, 32, 256)    0           bn2b_branch2c[0][0]              \n",
      "                                                                 activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (32, 32, 32, 256)    0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2a (Conv2D)         (32, 32, 32, 64)     16384       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2a (BatchNormalizati (32, 32, 32, 64)     256         res2c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (32, 32, 32, 64)     0           bn2c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2b (Conv2D)         (32, 32, 32, 64)     36864       activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2b (BatchNormalizati (32, 32, 32, 64)     256         res2c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (32, 32, 32, 64)     0           bn2c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2c (Conv2D)         (32, 32, 32, 256)    16384       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2c (BatchNormalizati (32, 32, 32, 256)    1024        res2c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (32, 32, 32, 256)    0           bn2c_branch2c[0][0]              \n",
      "                                                                 activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (32, 32, 32, 256)    0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2a (Conv2D)         (32, 32, 32, 128)    32768       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2a (BatchNormalizati (32, 32, 32, 128)    512         res3a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (32, 32, 32, 128)    0           bn3a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2b (Conv2D)         (32, 16, 16, 128)    147456      activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2b (BatchNormalizati (32, 16, 16, 128)    512         res3a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (32, 16, 16, 128)    0           bn3a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2c (Conv2D)         (32, 16, 16, 512)    65536       activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch1 (Conv2D)          (32, 16, 16, 512)    131072      activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2c (BatchNormalizati (32, 16, 16, 512)    2048        res3a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch1 (BatchNormalizatio (32, 16, 16, 512)    2048        res3a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (32, 16, 16, 512)    0           bn3a_branch2c[0][0]              \n",
      "                                                                 bn3a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (32, 16, 16, 512)    0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2a (Conv2D)         (32, 16, 16, 128)    65536       activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2a (BatchNormalizati (32, 16, 16, 128)    512         res3b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (32, 16, 16, 128)    0           bn3b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2b (Conv2D)         (32, 16, 16, 128)    147456      activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2b (BatchNormalizati (32, 16, 16, 128)    512         res3b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (32, 16, 16, 128)    0           bn3b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2c (Conv2D)         (32, 16, 16, 512)    65536       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2c (BatchNormalizati (32, 16, 16, 512)    2048        res3b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (32, 16, 16, 512)    0           bn3b_branch2c[0][0]              \n",
      "                                                                 activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (32, 16, 16, 512)    0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2a (Conv2D)         (32, 16, 16, 128)    65536       activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2a (BatchNormalizati (32, 16, 16, 128)    512         res3c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (32, 16, 16, 128)    0           bn3c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2b (Conv2D)         (32, 16, 16, 128)    147456      activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2b (BatchNormalizati (32, 16, 16, 128)    512         res3c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (32, 16, 16, 128)    0           bn3c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2c (Conv2D)         (32, 16, 16, 512)    65536       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2c (BatchNormalizati (32, 16, 16, 512)    2048        res3c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (32, 16, 16, 512)    0           bn3c_branch2c[0][0]              \n",
      "                                                                 activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (32, 16, 16, 512)    0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2a (Conv2D)         (32, 16, 16, 128)    65536       activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2a (BatchNormalizati (32, 16, 16, 128)    512         res3d_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (32, 16, 16, 128)    0           bn3d_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2b (Conv2D)         (32, 16, 16, 128)    147456      activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2b (BatchNormalizati (32, 16, 16, 128)    512         res3d_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (32, 16, 16, 128)    0           bn3d_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2c (Conv2D)         (32, 16, 16, 512)    65536       activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2c (BatchNormalizati (32, 16, 16, 512)    2048        res3d_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (32, 16, 16, 512)    0           bn3d_branch2c[0][0]              \n",
      "                                                                 activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (32, 16, 16, 512)    0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2a (Conv2D)         (32, 16, 16, 256)    131072      activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2a (BatchNormalizati (32, 16, 16, 256)    1024        res4a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (32, 16, 16, 256)    0           bn4a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2b (Conv2D)         (32, 8, 8, 256)      589824      activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2b (BatchNormalizati (32, 8, 8, 256)      1024        res4a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (32, 8, 8, 256)      0           bn4a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2c (Conv2D)         (32, 8, 8, 1024)     262144      activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch1 (Conv2D)          (32, 8, 8, 1024)     524288      activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2c (BatchNormalizati (32, 8, 8, 1024)     4096        res4a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch1 (BatchNormalizatio (32, 8, 8, 1024)     4096        res4a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (32, 8, 8, 1024)     0           bn4a_branch2c[0][0]              \n",
      "                                                                 bn4a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (32, 8, 8, 1024)     0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2a (Conv2D)         (32, 8, 8, 256)      262144      activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2a (BatchNormalizati (32, 8, 8, 256)      1024        res4b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (32, 8, 8, 256)      0           bn4b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2b (Conv2D)         (32, 8, 8, 256)      589824      activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2b (BatchNormalizati (32, 8, 8, 256)      1024        res4b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (32, 8, 8, 256)      0           bn4b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2c (Conv2D)         (32, 8, 8, 1024)     262144      activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2c (BatchNormalizati (32, 8, 8, 1024)     4096        res4b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (32, 8, 8, 1024)     0           bn4b_branch2c[0][0]              \n",
      "                                                                 activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (32, 8, 8, 1024)     0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2a (Conv2D)         (32, 8, 8, 256)      262144      activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4c_branch2a (BatchNormalizati (32, 8, 8, 256)      1024        res4c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (32, 8, 8, 256)      0           bn4c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2b (Conv2D)         (32, 8, 8, 256)      589824      activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4c_branch2b (BatchNormalizati (32, 8, 8, 256)      1024        res4c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (32, 8, 8, 256)      0           bn4c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2c (Conv2D)         (32, 8, 8, 1024)     262144      activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4c_branch2c (BatchNormalizati (32, 8, 8, 1024)     4096        res4c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (32, 8, 8, 1024)     0           bn4c_branch2c[0][0]              \n",
      "                                                                 activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (32, 8, 8, 1024)     0           add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2a (Conv2D)         (32, 8, 8, 256)      262144      activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4d_branch2a (BatchNormalizati (32, 8, 8, 256)      1024        res4d_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (32, 8, 8, 256)      0           bn4d_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2b (Conv2D)         (32, 8, 8, 256)      589824      activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4d_branch2b (BatchNormalizati (32, 8, 8, 256)      1024        res4d_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (32, 8, 8, 256)      0           bn4d_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2c (Conv2D)         (32, 8, 8, 1024)     262144      activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4d_branch2c (BatchNormalizati (32, 8, 8, 1024)     4096        res4d_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (32, 8, 8, 1024)     0           bn4d_branch2c[0][0]              \n",
      "                                                                 activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (32, 8, 8, 1024)     0           add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2a (Conv2D)         (32, 8, 8, 256)      262144      activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4e_branch2a (BatchNormalizati (32, 8, 8, 256)      1024        res4e_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (32, 8, 8, 256)      0           bn4e_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2b (Conv2D)         (32, 8, 8, 256)      589824      activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4e_branch2b (BatchNormalizati (32, 8, 8, 256)      1024        res4e_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (32, 8, 8, 256)      0           bn4e_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2c (Conv2D)         (32, 8, 8, 1024)     262144      activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4e_branch2c (BatchNormalizati (32, 8, 8, 1024)     4096        res4e_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (32, 8, 8, 1024)     0           bn4e_branch2c[0][0]              \n",
      "                                                                 activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (32, 8, 8, 1024)     0           add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2a (Conv2D)         (32, 8, 8, 256)      262144      activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4f_branch2a (BatchNormalizati (32, 8, 8, 256)      1024        res4f_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (32, 8, 8, 256)      0           bn4f_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2b (Conv2D)         (32, 8, 8, 256)      589824      activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4f_branch2b (BatchNormalizati (32, 8, 8, 256)      1024        res4f_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (32, 8, 8, 256)      0           bn4f_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2c (Conv2D)         (32, 8, 8, 1024)     262144      activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4f_branch2c (BatchNormalizati (32, 8, 8, 1024)     4096        res4f_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (32, 8, 8, 1024)     0           bn4f_branch2c[0][0]              \n",
      "                                                                 activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (32, 8, 8, 1024)     0           add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2a (Conv2D)         (32, 8, 8, 512)      524288      activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch2a (BatchNormalizati (32, 8, 8, 512)      2048        res5a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (32, 8, 8, 512)      0           bn5a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2b (Conv2D)         (32, 4, 4, 512)      2359296     activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch2b (BatchNormalizati (32, 4, 4, 512)      2048        res5a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (32, 4, 4, 512)      0           bn5a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2c (Conv2D)         (32, 4, 4, 2048)     1048576     activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch1 (Conv2D)          (32, 4, 4, 2048)     2097152     activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch2c (BatchNormalizati (32, 4, 4, 2048)     8192        res5a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch1 (BatchNormalizatio (32, 4, 4, 2048)     8192        res5a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (32, 4, 4, 2048)     0           bn5a_branch2c[0][0]              \n",
      "                                                                 bn5a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (32, 4, 4, 2048)     0           add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2a (Conv2D)         (32, 4, 4, 512)      1048576     activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5b_branch2a (BatchNormalizati (32, 4, 4, 512)      2048        res5b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (32, 4, 4, 512)      0           bn5b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2b (Conv2D)         (32, 4, 4, 512)      2359296     activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5b_branch2b (BatchNormalizati (32, 4, 4, 512)      2048        res5b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (32, 4, 4, 512)      0           bn5b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2c (Conv2D)         (32, 4, 4, 2048)     1048576     activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5b_branch2c (BatchNormalizati (32, 4, 4, 2048)     8192        res5b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (32, 4, 4, 2048)     0           bn5b_branch2c[0][0]              \n",
      "                                                                 activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (32, 4, 4, 2048)     0           add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2a (Conv2D)         (32, 4, 4, 512)      1048576     activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5c_branch2a (BatchNormalizati (32, 4, 4, 512)      2048        res5c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (32, 4, 4, 512)      0           bn5c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2b (Conv2D)         (32, 4, 4, 512)      2359296     activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5c_branch2b (BatchNormalizati (32, 4, 4, 512)      2048        res5c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (32, 4, 4, 512)      0           bn5c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2c (Conv2D)         (32, 4, 4, 2048)     1048576     activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5c_branch2c (BatchNormalizati (32, 4, 4, 2048)     8192        res5c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_15 (Add)                    (32, 4, 4, 2048)     0           bn5c_branch2c[0][0]              \n",
      "                                                                 activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (32, 4, 4, 2048)     0           add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "reduce_mean (Lambda)            (32, 2048)           0           activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "fc1000 (Dense)                  (32, 10)             20490       reduce_mean[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (32, 10)             0           fc1000[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 23,575,370\n",
      "Trainable params: 23,522,250\n",
      "Non-trainable params: 53,120\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "m2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU-NEW",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
