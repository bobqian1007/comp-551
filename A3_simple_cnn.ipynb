{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 128, 128, 1)\n",
      "Number of images in x_train 50000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_images = pd.read_pickle('D:/Study/Fall2019/COMP551/551A3/train_max_x.zip') \n",
    "x_train = np.reshape(train_images, (50000, 128, 128))\n",
    "x_train = x_train.reshape(x_train.shape[0], 128, 128, 1)\n",
    "\n",
    "df = pd.read_csv('D:/Study/Fall2019/COMP551/551A3/train_y_mod.csv')\n",
    "y_train = df.Label.to_numpy()\n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Reshaping the array to 4-dims so that it can work with the Keras API\n",
    "#X_train = X_train.reshape(X_train.shape[0], 128, 128, 1)\n",
    "#X_test = X_test.reshape(X_test.shape[0], 128, 128, 1)\n",
    "input_shape = (128, 128, 1)\n",
    "# Making sure that the values are float so that we can get decimal points after division\n",
    "x_train = x_train.astype('float32')\n",
    "#X_test = X_test.astype('float32')\n",
    "# Normalizing the RGB codes by dividing it to the max RGB value.\n",
    "x_train /= 255.0\n",
    "#X_test /= 255.0\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('Number of images in x_train', x_train.shape[0])\n",
    "#print('Number of images in x_test', X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train = x_train.astype('float32')\n",
    "#x_train /= 255.0\n",
    "x_train[x_train < 0.8] = 0\n",
    "x_train[x_train >= 0.8] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from tensorflow.python.keras import backend\n",
    "from tensorflow.python.keras import initializers\n",
    "from tensorflow.python.keras import layers\n",
    "from tensorflow.python.keras import models\n",
    "from tensorflow.python.keras import regularizers\n",
    "#from official.vision.image_classification import imagenet_preprocessing\n",
    "\n",
    "L2_WEIGHT_DECAY = 1e-4\n",
    "BATCH_NORM_DECAY = 0.9\n",
    "BATCH_NORM_EPSILON = 1e-5\n",
    "\n",
    "\n",
    "def _gen_l2_regularizer(use_l2_regularizer=True):\n",
    "  return regularizers.l2(L2_WEIGHT_DECAY) if use_l2_regularizer else None\n",
    "\n",
    "\n",
    "def identity_block(input_tensor,\n",
    "                   kernel_size,\n",
    "                   filters,\n",
    "                   stage,\n",
    "                   block,\n",
    "                   use_l2_regularizer=True):\n",
    "  \"\"\"The identity block is the block that has no conv layer at shortcut.\n",
    "  Args:\n",
    "    input_tensor: input tensor\n",
    "    kernel_size: default 3, the kernel size of middle conv layer at main path\n",
    "    filters: list of integers, the filters of 3 conv layer at main path\n",
    "    stage: integer, current stage label, used for generating layer names\n",
    "    block: 'a','b'..., current block label, used for generating layer names\n",
    "    use_l2_regularizer: whether to use L2 regularizer on Conv layer.\n",
    "  Returns:\n",
    "    Output tensor for the block.\n",
    "  \"\"\"\n",
    "  filters1, filters2, filters3 = filters\n",
    "  if backend.image_data_format() == 'channels_last':\n",
    "    bn_axis = 3\n",
    "  else:\n",
    "    bn_axis = 1\n",
    "  conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "  bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "\n",
    "  x = layers.Conv2D(\n",
    "      filters1, (1, 1),\n",
    "      use_bias=False,\n",
    "      kernel_initializer='he_normal',\n",
    "      kernel_regularizer=_gen_l2_regularizer(use_l2_regularizer),\n",
    "      name=conv_name_base + '2a')(\n",
    "          input_tensor)\n",
    "  x = layers.BatchNormalization(\n",
    "      axis=bn_axis,\n",
    "      momentum=BATCH_NORM_DECAY,\n",
    "      epsilon=BATCH_NORM_EPSILON,\n",
    "      name=bn_name_base + '2a')(\n",
    "          x)\n",
    "  x = layers.Activation('relu')(x)\n",
    "\n",
    "  x = layers.Conv2D(\n",
    "      filters2,\n",
    "      kernel_size,\n",
    "      padding='same',\n",
    "      use_bias=False,\n",
    "      kernel_initializer='he_normal',\n",
    "      kernel_regularizer=_gen_l2_regularizer(use_l2_regularizer),\n",
    "      name=conv_name_base + '2b')(\n",
    "          x)\n",
    "  x = layers.BatchNormalization(\n",
    "      axis=bn_axis,\n",
    "      momentum=BATCH_NORM_DECAY,\n",
    "      epsilon=BATCH_NORM_EPSILON,\n",
    "      name=bn_name_base + '2b')(\n",
    "          x)\n",
    "  x = layers.Activation('relu')(x)\n",
    "\n",
    "  x = layers.Conv2D(\n",
    "      filters3, (1, 1),\n",
    "      use_bias=False,\n",
    "      kernel_initializer='he_normal',\n",
    "      kernel_regularizer=_gen_l2_regularizer(use_l2_regularizer),\n",
    "      name=conv_name_base + '2c')(\n",
    "          x)\n",
    "  x = layers.BatchNormalization(\n",
    "      axis=bn_axis,\n",
    "      momentum=BATCH_NORM_DECAY,\n",
    "      epsilon=BATCH_NORM_EPSILON,\n",
    "      name=bn_name_base + '2c')(\n",
    "          x)\n",
    "\n",
    "  x = layers.add([x, input_tensor])\n",
    "  x = layers.Activation('relu')(x)\n",
    "  return x\n",
    "\n",
    "\n",
    "def conv_block(input_tensor,\n",
    "               kernel_size,\n",
    "               filters,\n",
    "               stage,\n",
    "               block,\n",
    "               strides=(2, 2),\n",
    "               use_l2_regularizer=True):\n",
    "  \"\"\"A block that has a conv layer at shortcut.\n",
    "  Note that from stage 3,\n",
    "  the second conv layer at main path is with strides=(2, 2)\n",
    "  And the shortcut should have strides=(2, 2) as well\n",
    "  Args:\n",
    "    input_tensor: input tensor\n",
    "    kernel_size: default 3, the kernel size of middle conv layer at main path\n",
    "    filters: list of integers, the filters of 3 conv layer at main path\n",
    "    stage: integer, current stage label, used for generating layer names\n",
    "    block: 'a','b'..., current block label, used for generating layer names\n",
    "    strides: Strides for the second conv layer in the block.\n",
    "    use_l2_regularizer: whether to use L2 regularizer on Conv layer.\n",
    "  Returns:\n",
    "    Output tensor for the block.\n",
    "  \"\"\"\n",
    "  filters1, filters2, filters3 = filters\n",
    "  if backend.image_data_format() == 'channels_last':\n",
    "    bn_axis = 3\n",
    "  else:\n",
    "    bn_axis = 1\n",
    "  conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "  bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "\n",
    "  x = layers.Conv2D(\n",
    "      filters1, (1, 1),\n",
    "      use_bias=False,\n",
    "      kernel_initializer='he_normal',\n",
    "      kernel_regularizer=_gen_l2_regularizer(use_l2_regularizer),\n",
    "      name=conv_name_base + '2a')(\n",
    "          input_tensor)\n",
    "  x = layers.BatchNormalization(\n",
    "      axis=bn_axis,\n",
    "      momentum=BATCH_NORM_DECAY,\n",
    "      epsilon=BATCH_NORM_EPSILON,\n",
    "      name=bn_name_base + '2a')(\n",
    "          x)\n",
    "  x = layers.Activation('relu')(x)\n",
    "\n",
    "  x = layers.Conv2D(\n",
    "      filters2,\n",
    "      kernel_size,\n",
    "      strides=strides,\n",
    "      padding='same',\n",
    "      use_bias=False,\n",
    "      kernel_initializer='he_normal',\n",
    "      kernel_regularizer=_gen_l2_regularizer(use_l2_regularizer),\n",
    "      name=conv_name_base + '2b')(\n",
    "          x)\n",
    "  x = layers.BatchNormalization(\n",
    "      axis=bn_axis,\n",
    "      momentum=BATCH_NORM_DECAY,\n",
    "      epsilon=BATCH_NORM_EPSILON,\n",
    "      name=bn_name_base + '2b')(\n",
    "          x)\n",
    "  x = layers.Activation('relu')(x)\n",
    "\n",
    "  x = layers.Conv2D(\n",
    "      filters3, (1, 1),\n",
    "      use_bias=False,\n",
    "      kernel_initializer='he_normal',\n",
    "      kernel_regularizer=_gen_l2_regularizer(use_l2_regularizer),\n",
    "      name=conv_name_base + '2c')(\n",
    "          x)\n",
    "  x = layers.BatchNormalization(\n",
    "      axis=bn_axis,\n",
    "      momentum=BATCH_NORM_DECAY,\n",
    "      epsilon=BATCH_NORM_EPSILON,\n",
    "      name=bn_name_base + '2c')(\n",
    "          x)\n",
    "\n",
    "  shortcut = layers.Conv2D(\n",
    "      filters3, (1, 1),\n",
    "      strides=strides,\n",
    "      use_bias=False,\n",
    "      kernel_initializer='he_normal',\n",
    "      kernel_regularizer=_gen_l2_regularizer(use_l2_regularizer),\n",
    "      name=conv_name_base + '1')(\n",
    "          input_tensor)\n",
    "  shortcut = layers.BatchNormalization(\n",
    "      axis=bn_axis,\n",
    "      momentum=BATCH_NORM_DECAY,\n",
    "      epsilon=BATCH_NORM_EPSILON,\n",
    "      name=bn_name_base + '1')(\n",
    "          shortcut)\n",
    "\n",
    "  x = layers.add([x, shortcut])\n",
    "  x = layers.Activation('relu')(x)\n",
    "  return x\n",
    "\n",
    "\n",
    "def resnet50(num_classes,\n",
    "             batch_size=None,\n",
    "             use_l2_regularizer=True,\n",
    "             rescale_inputs=False):\n",
    "  \"\"\"Instantiates the ResNet50 architecture.\n",
    "  Args:\n",
    "    num_classes: `int` number of classes for image classification.\n",
    "    batch_size: Size of the batches for each step.\n",
    "    use_l2_regularizer: whether to use L2 regularizer on Conv/Dense layer.\n",
    "    rescale_inputs: whether to rescale inputs from 0 to 1.\n",
    "  Returns:\n",
    "      A Keras model instance.\n",
    "  \"\"\"\n",
    "  input_shape = (128, 128, 1)\n",
    "  img_input = layers.Input(shape=input_shape, batch_size=batch_size)\n",
    "  if rescale_inputs:\n",
    "    # Hub image modules expect inputs in the range [0, 1]. This rescales these\n",
    "    # inputs to the range expected by the trained model.\n",
    "    x = layers.Lambda(\n",
    "        lambda x: x * 255.0 - backend.constant(\n",
    "            imagenet_preprocessing.CHANNEL_MEANS,\n",
    "            shape=[1, 1, 3],\n",
    "            dtype=x.dtype),\n",
    "        name='rescale')(\n",
    "            img_input)\n",
    "  else:\n",
    "    x = img_input\n",
    "\n",
    "  if backend.image_data_format() == 'channels_first':\n",
    "    x = layers.Lambda(\n",
    "        lambda x: backend.permute_dimensions(x, (0, 3, 1, 2)),\n",
    "        name='transpose')(x)\n",
    "    bn_axis = 1\n",
    "  else:  # channels_last\n",
    "    bn_axis = 3\n",
    "\n",
    "  x = layers.ZeroPadding2D(padding=(3, 3), name='conv1_pad')(x)\n",
    "  x = layers.Conv2D(\n",
    "      64, (7, 7),\n",
    "      strides=(2, 2),\n",
    "      padding='valid',\n",
    "      use_bias=False,\n",
    "      kernel_initializer='he_normal',\n",
    "      kernel_regularizer=_gen_l2_regularizer(use_l2_regularizer),\n",
    "      name='conv1')(\n",
    "          x)\n",
    "  x = layers.BatchNormalization(\n",
    "      axis=bn_axis,\n",
    "      momentum=BATCH_NORM_DECAY,\n",
    "      epsilon=BATCH_NORM_EPSILON,\n",
    "      name='bn_conv1')(\n",
    "          x)\n",
    "  x = layers.Activation('relu')(x)\n",
    "  x = layers.MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n",
    "\n",
    "  x = conv_block(\n",
    "      x,\n",
    "      3, [64, 64, 256],\n",
    "      stage=2,\n",
    "      block='a',\n",
    "      strides=(1, 1),\n",
    "      use_l2_regularizer=use_l2_regularizer)\n",
    "  x = identity_block(\n",
    "      x,\n",
    "      3, [64, 64, 256],\n",
    "      stage=2,\n",
    "      block='b',\n",
    "      use_l2_regularizer=use_l2_regularizer)\n",
    "  x = identity_block(\n",
    "      x,\n",
    "      3, [64, 64, 256],\n",
    "      stage=2,\n",
    "      block='c',\n",
    "      use_l2_regularizer=use_l2_regularizer)\n",
    "\n",
    "  x = conv_block(\n",
    "      x,\n",
    "      3, [128, 128, 512],\n",
    "      stage=3,\n",
    "      block='a',\n",
    "      use_l2_regularizer=use_l2_regularizer)\n",
    "  x = identity_block(\n",
    "      x,\n",
    "      3, [128, 128, 512],\n",
    "      stage=3,\n",
    "      block='b',\n",
    "      use_l2_regularizer=use_l2_regularizer)\n",
    "  x = identity_block(\n",
    "      x,\n",
    "      3, [128, 128, 512],\n",
    "      stage=3,\n",
    "      block='c',\n",
    "      use_l2_regularizer=use_l2_regularizer)\n",
    "  x = identity_block(\n",
    "      x,\n",
    "      3, [128, 128, 512],\n",
    "      stage=3,\n",
    "      block='d',\n",
    "      use_l2_regularizer=use_l2_regularizer)\n",
    "\n",
    "  x = conv_block(\n",
    "      x,\n",
    "      3, [256, 256, 1024],\n",
    "      stage=4,\n",
    "      block='a',\n",
    "      use_l2_regularizer=use_l2_regularizer)\n",
    "  x = identity_block(\n",
    "      x,\n",
    "      3, [256, 256, 1024],\n",
    "      stage=4,\n",
    "      block='b',\n",
    "      use_l2_regularizer=use_l2_regularizer)\n",
    "  x = identity_block(\n",
    "      x,\n",
    "      3, [256, 256, 1024],\n",
    "      stage=4,\n",
    "      block='c',\n",
    "      use_l2_regularizer=use_l2_regularizer)\n",
    "  x = identity_block(\n",
    "      x,\n",
    "      3, [256, 256, 1024],\n",
    "      stage=4,\n",
    "      block='d',\n",
    "      use_l2_regularizer=use_l2_regularizer)\n",
    "  x = identity_block(\n",
    "      x,\n",
    "      3, [256, 256, 1024],\n",
    "      stage=4,\n",
    "      block='e',\n",
    "      use_l2_regularizer=use_l2_regularizer)\n",
    "  x = identity_block(\n",
    "      x,\n",
    "      3, [256, 256, 1024],\n",
    "      stage=4,\n",
    "      block='f',\n",
    "      use_l2_regularizer=use_l2_regularizer)\n",
    "\n",
    "  x = conv_block(\n",
    "      x,\n",
    "      3, [512, 512, 2048],\n",
    "      stage=5,\n",
    "      block='a',\n",
    "      use_l2_regularizer=use_l2_regularizer)\n",
    "  x = identity_block(\n",
    "      x,\n",
    "      3, [512, 512, 2048],\n",
    "      stage=5,\n",
    "      block='b',\n",
    "      use_l2_regularizer=use_l2_regularizer)\n",
    "  x = identity_block(\n",
    "      x,\n",
    "      3, [512, 512, 2048],\n",
    "      stage=5,\n",
    "      block='c',\n",
    "      use_l2_regularizer=use_l2_regularizer)\n",
    "\n",
    "  rm_axes = [1, 2] if backend.image_data_format() == 'channels_last' else [2, 3]\n",
    "  x = layers.Lambda(lambda x: backend.mean(x, rm_axes), name='reduce_mean')(x)\n",
    "  x = layers.Dense(\n",
    "      num_classes,\n",
    "      kernel_initializer=initializers.RandomNormal(stddev=0.01),\n",
    "      kernel_regularizer=_gen_l2_regularizer(use_l2_regularizer),\n",
    "      bias_regularizer=_gen_l2_regularizer(use_l2_regularizer),\n",
    "      name='fc1000')(\n",
    "          x)\n",
    "\n",
    "  # A softmax that is followed by the model loss must be done cannot be done\n",
    "  # in float16 due to numeric issues. So we pass dtype=float32.\n",
    "  x = layers.Activation('softmax', dtype='float32')(x)\n",
    "\n",
    "  # Create model.\n",
    "  return models.Model(img_input, x, name='resnet50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda\\AnacondaInstall\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\initializers.py:143: calling RandomNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "model = resnet50(10,\n",
    "             batch_size=32,\n",
    "             use_l2_regularizer=True,\n",
    "             rescale_inputs=False)\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', \n",
    "              loss='sparse_categorical_crossentropy', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "49984/49984 [==============================] - 190s 4ms/sample - loss: 3.2434 - acc: 0.6118\n",
      "49984/49984 [==============================] - 183s 4ms/sample - loss: 1.1058 - acc: 0.8482\n",
      "epoch 1\n",
      "49984/49984 [==============================] - 184s 4ms/sample - loss: 0.8286 - acc: 0.8887\n",
      "49984/49984 [==============================] - 184s 4ms/sample - loss: 0.7357 - acc: 0.9071\n",
      "epoch 2\n",
      "49984/49984 [==============================] - 183s 4ms/sample - loss: 0.6917 - acc: 0.9125\n",
      "26976/49984 [===============>..............] - ETA: 1:24 - loss: 0.6197 - acc: 0.9252"
     ]
    }
   ],
   "source": [
    "for i in range(15):\n",
    "    print(\"epoch\", i)\n",
    "    model.fit(x=x_train[0:49984] ,y=y_train[0:49984], epochs=1, batch_size = 32)\n",
    "    model.fit(x=x_train[16:50000] ,y=y_train[16:50000], epochs=1, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "49984/49984 [==============================] - 181s 4ms/sample - loss: 0.2497 - acc: 0.9713\n",
      "Epoch 2/5\n",
      "49984/49984 [==============================] - 182s 4ms/sample - loss: 0.2531 - acc: 0.9711\n",
      "Epoch 3/5\n",
      "49984/49984 [==============================] - 182s 4ms/sample - loss: 0.2462 - acc: 0.9712\n",
      "Epoch 4/5\n",
      "49984/49984 [==============================] - 182s 4ms/sample - loss: 0.2423 - acc: 0.9719\n",
      "Epoch 5/5\n",
      "49984/49984 [==============================] - 182s 4ms/sample - loss: 0.2399 - acc: 0.9723\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2d2b1fef688>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=x_train[0:49984] ,y=y_train[0:49984], epochs=5, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "49984/49984 [==============================] - 182s 4ms/sample - loss: 0.2349 - acc: 0.9725\n",
      "Epoch 2/5\n",
      "49984/49984 [==============================] - 183s 4ms/sample - loss: 0.2255 - acc: 0.9760\n",
      "Epoch 3/5\n",
      "49984/49984 [==============================] - 183s 4ms/sample - loss: 0.2280 - acc: 0.9738\n",
      "Epoch 4/5\n",
      "49984/49984 [==============================] - 183s 4ms/sample - loss: 0.2325 - acc: 0.9722\n",
      "Epoch 5/5\n",
      "49984/49984 [==============================] - 182s 4ms/sample - loss: 0.2262 - acc: 0.9747\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2d30f5bca48>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=x_train[0:49984] ,y=y_train[0:49984], epochs=5, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49984/49984 [==============================] - 182s 4ms/sample - loss: 0.2226 - acc: 0.9753\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2d3127d18c8>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=x_train[0:49984] ,y=y_train[0:49984], epochs=1, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_images = pd.read_pickle('D:/Study/Fall2019/COMP551/551A3/test_max_x.zip') \n",
    "x_test = np.reshape(result_images, (10000, 128, 128))\n",
    "x_test = np.reshape(x_test, (10000, 128, 128, 1))\n",
    "\n",
    "x_test = x_test.astype('float32')\n",
    "x_test /= 255.0\n",
    "\n",
    "x_test[x_test < 0.8] = 0\n",
    "x_test[x_test >= 0.8] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_result = model.predict(x=x_test)\n",
    "result = []\n",
    "for i in range(10000):\n",
    "    result.append(np.argmax(y_result[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('D:/Study/Fall2019/COMP551/551A3/submitC.csv', mode='w') as writingFile:\n",
    "    writer = csv.writer(writingFile)\n",
    "    writer.writerow(['id', 'Label'])\n",
    "    for index in range(10000):\n",
    "        writer.writerow([index, result[index]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "49984/49984 [==============================] - 180s 4ms/sample - loss: 0.2217 - acc: 0.9745\n",
      "Epoch 2/15\n",
      "49984/49984 [==============================] - 182s 4ms/sample - loss: 0.2193 - acc: 0.9757\n",
      "Epoch 3/15\n",
      "49984/49984 [==============================] - 182s 4ms/sample - loss: 0.2199 - acc: 0.9752\n",
      "Epoch 4/15\n",
      "49984/49984 [==============================] - 182s 4ms/sample - loss: 0.2158 - acc: 0.9758\n",
      "Epoch 5/15\n",
      "49984/49984 [==============================] - 182s 4ms/sample - loss: 0.2185 - acc: 0.9754\n",
      "Epoch 6/15\n",
      "49984/49984 [==============================] - 182s 4ms/sample - loss: 0.2182 - acc: 0.9750\n",
      "Epoch 7/15\n",
      "49984/49984 [==============================] - 182s 4ms/sample - loss: 0.2109 - acc: 0.9770\n",
      "Epoch 8/15\n",
      "49984/49984 [==============================] - 182s 4ms/sample - loss: 0.2092 - acc: 0.9767\n",
      "Epoch 9/15\n",
      "49984/49984 [==============================] - 183s 4ms/sample - loss: 0.2117 - acc: 0.9759\n",
      "Epoch 10/15\n",
      "49984/49984 [==============================] - 182s 4ms/sample - loss: 0.2062 - acc: 0.9772\n",
      "Epoch 11/15\n",
      "49984/49984 [==============================] - 182s 4ms/sample - loss: 0.2013 - acc: 0.9782\n",
      "Epoch 12/15\n",
      "49984/49984 [==============================] - 182s 4ms/sample - loss: 0.2090 - acc: 0.9761\n",
      "Epoch 13/15\n",
      "49984/49984 [==============================] - 182s 4ms/sample - loss: 0.2026 - acc: 0.9778\n",
      "Epoch 14/15\n",
      "49984/49984 [==============================] - 182s 4ms/sample - loss: 0.2029 - acc: 0.9775\n",
      "Epoch 15/15\n",
      "49984/49984 [==============================] - 182s 4ms/sample - loss: 0.2048 - acc: 0.9766\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2d312920888>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=x_train[0:49984] ,y=y_train[0:49984], epochs=15, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "49984/49984 [==============================] - 181s 4ms/sample - loss: 0.1981 - acc: 0.9788\n",
      "Epoch 2/15\n",
      "49984/49984 [==============================] - 182s 4ms/sample - loss: 0.2004 - acc: 0.9777\n",
      "Epoch 3/15\n",
      "49984/49984 [==============================] - 183s 4ms/sample - loss: 0.1998 - acc: 0.9780\n",
      "Epoch 4/15\n",
      "49984/49984 [==============================] - 182s 4ms/sample - loss: 0.2008 - acc: 0.9776\n",
      "Epoch 5/15\n",
      "49984/49984 [==============================] - 182s 4ms/sample - loss: 0.1980 - acc: 0.9778\n",
      "Epoch 6/15\n",
      "49984/49984 [==============================] - 182s 4ms/sample - loss: 0.1900 - acc: 0.9802\n",
      "Epoch 7/15\n",
      "49984/49984 [==============================] - 182s 4ms/sample - loss: 0.2027 - acc: 0.9770\n",
      "Epoch 8/15\n",
      "49984/49984 [==============================] - 182s 4ms/sample - loss: 0.1889 - acc: 0.9803\n",
      "Epoch 9/15\n",
      "49984/49984 [==============================] - 182s 4ms/sample - loss: 0.1942 - acc: 0.9770\n",
      "Epoch 10/15\n",
      "49984/49984 [==============================] - 182s 4ms/sample - loss: 0.1921 - acc: 0.9791\n",
      "Epoch 11/15\n",
      "49984/49984 [==============================] - 182s 4ms/sample - loss: 0.1939 - acc: 0.9786\n",
      "Epoch 12/15\n",
      "49984/49984 [==============================] - 182s 4ms/sample - loss: 0.1859 - acc: 0.9799\n",
      "Epoch 13/15\n",
      "49984/49984 [==============================] - 182s 4ms/sample - loss: 0.1895 - acc: 0.9790\n",
      "Epoch 14/15\n",
      "49984/49984 [==============================] - 182s 4ms/sample - loss: 0.1870 - acc: 0.9798\n",
      "Epoch 15/15\n",
      "49984/49984 [==============================] - 182s 4ms/sample - loss: 0.1853 - acc: 0.9797\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2d31274d3c8>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=x_train[0:49984] ,y=y_train[0:49984], epochs=15, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x=x_train[0:49984] ,y=y_train[0:49984], epochs=5, batch_size = 32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU-NEW",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
